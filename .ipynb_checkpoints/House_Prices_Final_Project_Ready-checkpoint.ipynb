{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3b2de7a",
   "metadata": {},
   "source": [
    "## Загрузка данных и первичный осмотр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff722ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.read_csv('train (1).csv')\n",
    "test = pd.read_csv('test (1).csv')\n",
    "\n",
    "print(train.shape)\n",
    "print(train.head())\n",
    "print(train.describe())\n",
    "\n",
    "print(\"Количество пропусков:\")\n",
    "print(train.isnull().sum())\n",
    "\n",
    "# Я загрузил данные, посмотрел их структуру и статистику.\n",
    "# Я сразу увидел наличие пропусков, которые необходимо будет обработать.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a28754",
   "metadata": {},
   "source": [
    "## Удаление ненужных признаков и обработка пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954de91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_to_drop = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'Id']\n",
    "train_clean = train.drop(columns=cols_to_drop)\n",
    "\n",
    "categorical_cols = train_clean.select_dtypes(include=['object']).columns\n",
    "numerical_cols = train_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "train_clean[categorical_cols] = train_clean[categorical_cols].fillna('None')\n",
    "train_clean[numerical_cols] = train_clean[numerical_cols].fillna(train_clean[numerical_cols].median())\n",
    "\n",
    "print(\"Пропуски после обработки:\")\n",
    "print(train_clean.isnull().sum())\n",
    "\n",
    "# Я удалил признаки с большим количеством пропусков.\n",
    "# Остальные пропуски заполнил корректно.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb08b9",
   "metadata": {},
   "source": [
    "## Удаление выбросов по GrLivArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1 = train_clean['GrLivArea'].quantile(0.25)\n",
    "Q3 = train_clean['GrLivArea'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = train_clean[(train_clean['GrLivArea'] < (Q1 - 1.5 * IQR)) | (train_clean['GrLivArea'] > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "print(f\"Количество аномалий GrLivArea: {len(outliers)}\")\n",
    "\n",
    "train_clean = train_clean[~train_clean.index.isin(outliers.index)]\n",
    "\n",
    "# Я удалил выбросы по GrLivArea для более корректного анализа и обучения моделей.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06fab9",
   "metadata": {},
   "source": [
    "## Анализ распределения целевого признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.histplot(train_clean['SalePrice'], kde=True)\n",
    "plt.title('Распределение SalePrice')\n",
    "plt.show()\n",
    "\n",
    "# Я визуализировал распределение целевого признака и увидел наличие смещения вправо.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb2fe23",
   "metadata": {},
   "source": [
    "## Корреляционная матрица после чистки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_matrix = train_clean.select_dtypes(include=['int64', 'float64']).corr()\n",
    "corr_target = corr_matrix['SalePrice'].abs().sort_values(ascending=False).index\n",
    "sorted_corr_matrix = corr_matrix.loc[corr_target, corr_target]\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(sorted_corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', annot_kws={'size':7})\n",
    "plt.title('Корреляционная матрица (отсортирована по SalePrice)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Я построил корреляционную матрицу для анализа взаимосвязей после удаления выбросов.\n",
    "# Я определил наиболее важные признаки.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a3d75",
   "metadata": {},
   "source": [
    "## Кодирование категориальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "for col in train_clean.select_dtypes(include=['object']).columns:\n",
    "    train_clean[col] = encoder.fit_transform(train_clean[col].astype(str))\n",
    "\n",
    "# Я закодировал категориальные признаки для использования в моделях.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee16336",
   "metadata": {},
   "source": [
    "## Финальное разделение и стандартизация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = train_clean.drop(columns=['SalePrice'])\n",
    "y = train_clean['SalePrice']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Я подготовил обучающую и тестовую выборки, стандартизировал данные.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6b99f",
   "metadata": {},
   "source": [
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train_scaled, y_train)\n",
    "y_pred = model_lr.predict(X_test_scaled)\n",
    "\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred):.2f}')\n",
    "print(f'MAE: {mean_absolute_error(y_test, y_pred):.2f}')\n",
    "print(f'R2: {r2_score(y_test, y_pred):.2f}')\n",
    "\n",
    "# Я обучил линейную регрессию и оценил её качество.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb75891",
   "metadata": {},
   "source": [
    "## Дерево решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model_tree = DecisionTreeRegressor(random_state=42)\n",
    "model_tree.fit(X_train_scaled, y_train)\n",
    "y_pred_tree = model_tree.predict(X_test_scaled)\n",
    "\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred_tree):.2f}')\n",
    "print(f'MAE: {mean_absolute_error(y_test, y_pred_tree):.2f}')\n",
    "print(f'R2: {r2_score(y_test, y_pred_tree):.2f}')\n",
    "\n",
    "# Я обучил дерево решений и сравнил его метрики с линейной моделью.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e222d9e",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c4efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "model_xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model_xgb.fit(X_train_scaled, y_train)\n",
    "y_pred_xgb = model_xgb.predict(X_test_scaled)\n",
    "\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred_xgb):.2f}')\n",
    "print(f'MAE: {mean_absolute_error(y_test, y_pred_xgb):.2f}')\n",
    "print(f'R2: {r2_score(y_test, y_pred_xgb):.2f}')\n",
    "\n",
    "# Я обучил градиентный бустинг и получил лучшие результаты по сравнению с предыдущими моделями.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe066cd1",
   "metadata": {},
   "source": [
    "## Нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "model_mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=2000, random_state=42)\n",
    "model_mlp.fit(X_train_scaled, y_train)\n",
    "y_pred_mlp = model_mlp.predict(X_test_scaled)\n",
    "\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred_mlp):.2f}')\n",
    "print(f'MAE: {mean_absolute_error(y_test, y_pred_mlp):.2f}')\n",
    "print(f'R2: {r2_score(y_test, y_pred_mlp):.2f}')\n",
    "\n",
    "# Я обучил нейронную сеть и оценил её производительность.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb2dd5",
   "metadata": {},
   "source": [
    "## Кросс-валидация и финальный вывод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80af177",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "scores_xgb = cross_val_score(model_xgb, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "print(f'Среднее R2 XGBoost по 5 фолдам: {np.mean(scores_xgb):.2f}')\n",
    "\n",
    "# Я провёл кросс-валидацию для модели XGBoost.\n",
    "# Я подтверждаю, что эта модель показывает наилучший результат и является финальным выбором.\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
